{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Doog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "import csv\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 设置常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "DATA_PATH_stays= \"../data/ICUSTAYS.csv\"\n",
    "DATA_PATH_notes=\"../data/NOTEEVENTS.csv\" \n",
    "DATA_PATH_admission = \"../data/ADMISSIONS.csv\"\n",
    "\n",
    "DATA_PATH_train =\"../data/AKI/train_listfile.csv\" \n",
    "DATA_PATH_test = \"../data/AKI/test_listfile.csv\" \n",
    "DATA_PATH_validation = \"../data/AKI/val_listfile.csv\" \n",
    "\n",
    "OUTPUT_PATH_train = \"../data/train\"\n",
    "OUTPUT_PATH_test = \"../data/test\"\n",
    "\n",
    "SEPARATOR=\",\"\n",
    "CATEGORY_LIST = ['pad', 'Respiratory', 'ECG','Radiology','Nursing/other','Rehab Services','Nutrition','Pharmacy','Social Work','Case Management',\n",
    "            'Physician','General','Nursing','Echo','Consult']\n",
    "category_id = {cate: idx for idx, cate in enumerate(CATEGORY_LIST)}\n",
    "\n",
    "MAX_TIME = 48.0 # set max num of hours, in our case is 35(days) x 24 =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 固定随机种子，保证实验的可重复性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定随机种子, 保证可复现性\n",
    "def same_seed(seed = 42): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "same_seed(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 定义全局功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "def read_icustays_table(DATA_PATH_stays):\n",
    "    stays = pd.read_csv(DATA_PATH_stays, sep = SEPARATOR)\n",
    "    stays.INTIME = pd.to_datetime(stays.INTIME)\n",
    "    stays.OUTTIME = pd.to_datetime(stays.OUTTIME)\n",
    "    # drop the column that are not used in the future\n",
    "    stays = stays.drop(['ROW_ID', 'DBSOURCE', 'FIRST_CAREUNIT', 'LAST_CAREUNIT', 'FIRST_WARDID', 'LAST_WARDID'], axis=1)\n",
    "    return stays\n",
    "\n",
    "def read_notes_table(DATA_PATH_notes, keep_discharge=False):\n",
    "    # 读取 NOTEEVENTS.csv，跳过格式错误行\n",
    "    df_note = pd.read_csv(\n",
    "        DATA_PATH_notes,\n",
    "        sep=SEPARATOR,\n",
    "        engine='python',\n",
    "        encoding='utf-8',\n",
    "        on_bad_lines='skip',  # 替代已弃用的 error_bad_lines\n",
    "        quoting=csv.QUOTE_ALL\n",
    "    )\n",
    "\n",
    "    # 可选：去除 discharge summary 类别\n",
    "    if not keep_discharge:\n",
    "        df_note = df_note[df_note['CATEGORY'] != 'Discharge summary']\n",
    "    # 去除被标记为错误的笔记\n",
    "    df_note = df_note[df_note['ISERROR'] != 1]\n",
    "    # 删除无用列（若存在）\n",
    "    drop_cols = ['ROW_ID', 'STORETIME', 'DESCRIPTION', 'CGID', 'ISERROR']\n",
    "    df_note = df_note.drop(columns=drop_cols, errors='ignore')\n",
    "    return df_note\n",
    "\n",
    "def merge_on_subject_admission(table1):\n",
    "    admission = pd.read_csv(DATA_PATH_admission, sep = SEPARATOR)\n",
    "    # drop the column that are not used in the future\n",
    "    admission = admission.drop(['ROW_ID', \"ADMITTIME\",\"DEATHTIME\",\"ADMISSION_TYPE\",\"ADMISSION_LOCATION\", \"DISCHARGE_LOCATION\",\n",
    "                        \"INSURANCE\",\"LANGUAGE\",\"RELIGION\", \"MARITAL_STATUS\",\"ETHNICITY\",\"EDREGTIME\",\"EDOUTTIME\",\"HOSPITAL_EXPIRE_FLAG\",\"HAS_CHARTEVENTS_DATA\"], axis=1)\n",
    "    return table1.merge(admission, how='inner', left_on=['SUBJECT_ID', 'HADM_ID'], right_on=['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "\n",
    "def filter_notes_on_stays(notes, stays):\n",
    "    return pd.merge(notes, stays.drop_duplicates(), left_on=['SUBJECT_ID', 'HADM_ID'], right_on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    \n",
    "def create_train_val_test_notes(notes, subject_id_list, max_time=240):\n",
    "    \"\"\"\n",
    "    根据 subject_id_list 从 notes 中筛选相关笔记，计算相对 ICU 入院时间（HOURS），\n",
    "    并筛选指定时间窗内的记录，返回处理后的 notes。\n",
    "\n",
    "    参数：\n",
    "        notes : pd.DataFrame 包含 NOTE、INTIME、CHARTTIME 等字段\n",
    "        subject_id_list : list 要保留的 SUBJECT_ID\n",
    "        max_time : float 最大保留的时间跨度（单位：小时）\n",
    "\n",
    "    返回：\n",
    "        pd.DataFrame 处理后 notes 子集\n",
    "    \"\"\"\n",
    "    # 确保时间字段为 datetime 类型\n",
    "    notes['DISCHTIME'] = pd.to_datetime(notes['DISCHTIME'], errors='coerce')\n",
    "    notes['CHARTDATE'] = pd.to_datetime(notes['CHARTDATE'], errors='coerce')\n",
    "    notes['CHARTTIME'] = pd.to_datetime(notes['CHARTTIME'], errors='coerce')\n",
    "\n",
    "    # 根据天数截断 DISCHTIME → DISCHDATE（虽然本函数没用）\n",
    "    notes['DISCHDATE'] = notes['DISCHTIME'].values.astype('<M8[D]')\n",
    "\n",
    "    # 只保留目标 subject_id\n",
    "    notes = notes[notes['SUBJECT_ID'].isin(subject_id_list)].copy()\n",
    "    notes.sort_values(by=['SUBJECT_ID', 'CHARTTIME'], inplace=True, ignore_index=True)\n",
    "\n",
    "    # 计算相对时间差（单位：小时）\n",
    "    notes['HOURST'] = (notes['CHARTTIME'] - notes['INTIME']) / np.timedelta64(1, 'h')\n",
    "    notes['HOURSD'] = (notes['CHARTDATE'] - notes['INTIME']) / np.timedelta64(1, 'h')\n",
    "\n",
    "    # 优先使用 CHARTTIME 计算的小时数，若为空则用 CHARTDATE 计算的\n",
    "    notes['HOURS'] = notes['HOURST'].fillna(notes['HOURSD'])\n",
    "\n",
    "    # 筛选时间窗 [0, max_time]\n",
    "    notes = notes[(notes['HOURS'] >= 0) & (notes['HOURS'] <= max_time)]\n",
    "\n",
    "    # 清除中间计算列\n",
    "    notes.drop(columns=['HOURST', 'HOURSD', 'HOURS'], inplace=True, errors='ignore')\n",
    "\n",
    "    return notes\n",
    "\n",
    "def split_doc(d):\n",
    "    \"\"\"Split sentences in a document and saved the sentences to a list.\n",
    "\n",
    "    Args:\n",
    "        d: a document\n",
    "        final_d: a list of sentences\n",
    "    \"\"\"\n",
    "\n",
    "    d = d.strip().split(\".\") #d = d.strip().split(\".\") # split document by \".\" to sentences\n",
    "    final_d = []\n",
    "    for s in d:\n",
    "        if s != \"\":  # ignore if the sentence is empty\n",
    "            final_d.append(s.strip())\n",
    "    return final_d  # Now the sentences are splitted from documents and saved to a list\n",
    "\n",
    "#clean the sentence with some regex to delete some of the strange annotation like ---- **** etc.\n",
    "def _preprocess1(x):\n",
    "    y=re.sub('\\[(.*?)\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr.','doctor',y)\n",
    "    y=re.sub('m.d.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    return y\n",
    "\n",
    "def tokenize(sent, mimic3_embedding=None, nlp=None):\n",
    "    \"\"\"Tokenize the sentences according to the existing word from embedding.\n",
    "\n",
    "    Args:\n",
    "        sent: input a sentence\n",
    "        mimic3_embedding: find the existing word in embedding files\n",
    "        cleaned_tokens: the tokens are cleaned and mapped to the mimic embedding\n",
    "    \"\"\"\n",
    "\n",
    "    #tokenizer = re.compile('\\w+|\\*\\*|[^\\s\\w]')\n",
    "    tokens = nltk.word_tokenize(sent.lower())\n",
    "    #tokens = tokenizer.findall(sent.lower())\n",
    "    cleaned_tokens = []\n",
    "    for tok in tokens:\n",
    "        tok = _clean_token(tok)\n",
    "        if mimic3_embedding:\n",
    "            if tok in mimic3_embedding:\n",
    "                cleaned_tokens.append(tok)\n",
    "            else:\n",
    "                cleaned_tokens.append('UNK')\n",
    "        else:\n",
    "            cleaned_tokens.append(tok)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def _clean_token(s):\n",
    "    \"\"\"If the token is digit, then round the actual value into the nearest 10 times value.\n",
    "    Args:\n",
    "        s: original digit, 65 -> 60\n",
    "        \"\"\"\n",
    "    if len(s) > 1:\n",
    "        if s.isdigit():\n",
    "            l = len(s)\n",
    "            s = str(int(s)//(10**(l-1)) * 10**(l-1))\n",
    "    return s.lower()\n",
    "\n",
    "def break_up_notes_by_subject(notes, output_path, subjects=None, verbose=1):\n",
    "    subjects = notes.SUBJECT_ID.unique() if subjects is None else subjects\n",
    "    nb_subjects = subjects.shape[0]\n",
    "    notes = notes.drop_duplicates()\n",
    "    \n",
    "    #convert to Date time objects\n",
    "    notes['DISCHTIME'] = pd.to_datetime(notes['DISCHTIME'])   \n",
    "    notes['CHARTDATE'] = pd.to_datetime(notes['CHARTDATE'])\n",
    "    notes['CHARTTIME'] = pd.to_datetime(notes['CHARTTIME'])\n",
    "    notes['DISCHDATE'] = notes['DISCHTIME'].values.astype('<M8[D]')\n",
    "    \n",
    "    verbose = 1\n",
    "    output_path = output_path\n",
    "\n",
    "    for i, subject_id in enumerate(subjects):\n",
    "            if verbose:\n",
    "                sys.stdout.write('\\rSUBJECT {0} of {1}...'.format(i+1, nb_subjects))\n",
    "            dn = os.path.join(output_path, str(subject_id))\n",
    "            try:\n",
    "                os.makedirs(dn)\n",
    "            except:\n",
    "                pass\n",
    "            patient_note = notes.loc[notes.SUBJECT_ID == subject_id].sort_values(by=['CHARTDATE', 'CHARTTIME'])#.to_csv(os.path.join(dn, 'diagnoses.csv'), index=False)\n",
    "            intime = patient_note.INTIME\n",
    "\n",
    "            patient_note['HOURST'] = (patient_note.CHARTTIME - intime).apply(lambda s: s / np.timedelta64(1, 's')) / 60./60\n",
    "            patient_note['HOURSD'] = (patient_note.CHARTDATE - intime).apply(lambda s: s / np.timedelta64(1, 's')) / 60./60\n",
    "\n",
    "            note_filename = str(subject_id) + '_note.txt' \n",
    "            f = open(os.path.join(dn, note_filename), 'w')\n",
    "            j = 0\n",
    "            for doc, cat, c_date, c_time, ht, hd, icd_id, h_id in zip(patient_note['TEXT'], patient_note['CATEGORY'], patient_note['CHARTDATE'], patient_note['CHARTTIME'], patient_note['HOURST'], patient_note['HOURSD'], patient_note['ICUSTAY_ID'], patient_note['HADM_ID']): #'ICUSTAY_ID' \n",
    "                category = cat.strip() #d[0].strip()\n",
    "                category_index = category_id[category]\n",
    "                sentences = split_doc(doc)\n",
    "                #print(sentences)\n",
    "                #print(len(sentences))\n",
    "\n",
    "                \n",
    "                #filter notes written after 48.0 hours\n",
    "                #print(ht, hd)\n",
    "                if math.isnan(ht):\n",
    "                    h = hd\n",
    "                else:\n",
    "                    h = ht\n",
    "                \n",
    "                '''\n",
    "                if h >= 0. or h <= MAX_TIME:\n",
    "                    continue\n",
    "                '''\n",
    "                    \n",
    "                for sent in sentences:\n",
    "                #k=0\n",
    "                #if k < len(sentences):\n",
    "                    #sent = sentences[k]\n",
    "                    sent = _preprocess1(sent)\n",
    "                    #print(sent)\n",
    "                    cleaned_tokens = tokenize(sent)\n",
    "                    #print(cleaned_tokens)\n",
    "                    if len(cleaned_tokens) > 0:\n",
    "                        sent_head = '%s,%s,%s,%s,%s,%s,%s'%(str(j), str(category_index), c_date, c_time, str(h), str(icd_id), str(h_id))\n",
    "                        #print(sent_head + \"\\n\")\n",
    "                        f.write(sent_head + \"\\n\")\n",
    "                        for t in cleaned_tokens:\n",
    "                            #print(t + \"\\n\")\n",
    "                            f.write(t + \"\\n\")\n",
    "                        f.write(\"\\n\")\n",
    "                    #k+=1\n",
    "                j += 1\n",
    "            f.close()\n",
    "\n",
    "            \n",
    "    if verbose:\n",
    "        sys.stdout.write('DONE!\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#read csv file\n",
    "stays = read_icustays_table(DATA_PATH_stays)\n",
    "notes = read_notes_table(DATA_PATH_notes)\n",
    "notes = merge_on_subject_admission(notes)\n",
    "notes = filter_notes_on_stays(notes, stays)\n",
    "\n",
    "train_id = pd.read_csv(DATA_PATH_train, sep= SEPARATOR)\n",
    "test_id = pd.read_csv(DATA_PATH_test, sep= SEPARATOR) \n",
    "validation_id = pd.read_csv(DATA_PATH_validation, sep= SEPARATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 构造note的train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 从 listfile（train/test/val）中提取 subject_id 列表，\n",
    "就和`preprocessing.ipynb`的subject_id一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_ids(listfile_df):\n",
    "    return (\n",
    "        listfile_df['notes']\n",
    "        .apply(lambda x: int(x.split(\"_\")[0]))\n",
    "        .drop_duplicates()\n",
    "        .sort_values()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "train_list = extract_subject_ids(train_id)\n",
    "test_list = extract_subject_ids(test_id)\n",
    "val_list = extract_subject_ids(validation_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 从 notes 表中筛选指定 subject_id 的患者笔记记录\n",
    "统一标准化其时间戳字段，并计算相对于 ICU 入院时间（INTIME）的时间差（HOURS）用于建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notes_train = create_train_val_test_notes(notes, train_list)\n",
    "notes_test = create_train_val_test_notes(notes, test_list)\n",
    "notes_validation = create_train_val_test_notes(notes, val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 具有文本的 subject_id 少于具有特征的 subject_id，需要为模型重新创建新的 list.csv 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new train: 26865 subjects, old train: 27427 subjects\n",
      "new test: 3345 subjects, old test: 3428 subjects\n",
      "new val: 3363 subjects, old val: 3425 subjects\n"
     ]
    }
   ],
   "source": [
    "#new subject_id list\n",
    "new_train_list = notes_train.SUBJECT_ID.unique().tolist()\n",
    "print(\"new train: \" + str(len(new_train_list)) + \" subjects, old train: \" + str(len(train_list)) + \" subjects\")\n",
    "new_test_list = notes_test.SUBJECT_ID.unique().tolist()\n",
    "print(\"new test: \" + str(len(new_test_list)) + \" subjects, old test: \" + str(len(test_list)) + \" subjects\")\n",
    "new_val_list = notes_validation.SUBJECT_ID.unique().tolist()\n",
    "print(\"new val: \" + str(len(new_val_list)) + \" subjects, old val: \" + str(len(val_list)) + \" subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 构建新的train/val/test lifelist.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(\"../data/train\")\n",
    "    os.makedirs(\"../data/test\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_listfile_by_subject_ids(listfile_df, valid_subject_ids, output_path):\n",
    "    \"\"\"\n",
    "    从 listfile 中提取 subject_id, 并筛选出存在于 valid_subject_ids 的记录，保存为新的 listfile.csv\n",
    "\n",
    "    参数：\n",
    "        listfile_df : pd.DataFrame, 原始的 listfile DataFrame, 必须包含 'notes' 列\n",
    "        valid_subject_ids : list, 有效的 subject_id 列表\n",
    "        output_path : str, 要保存的新 listfile 文件路径\n",
    "    \"\"\"\n",
    "    # 提取 notes 文件名前缀作为 SUBJECT_ID\n",
    "    listfile_df['SUBJECT_ID'] = listfile_df['notes'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "\n",
    "    # 筛选在有效列表中的记录\n",
    "    filtered_df = (\n",
    "        listfile_df[listfile_df['SUBJECT_ID'].isin(valid_subject_ids)]\n",
    "        .sort_values(by='SUBJECT_ID')\n",
    "        .drop(columns='SUBJECT_ID')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # 保存为 CSV\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "    print(f\"已保存新 listfile 至: {output_path}（剩余 {len(filtered_df)} 条记录）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存新 listfile 至: ../data/train/train_listfile.csv（剩余 37150 条记录）\n",
      "已保存新 listfile 至: ../data/test/test_listfile.csv（剩余 4689 条记录）\n",
      "已保存新 listfile 至: ../data/train/val_listfile.csv（剩余 4656 条记录）\n"
     ]
    }
   ],
   "source": [
    "update_listfile_by_subject_ids(train_id, new_train_list, \"../data/train/train_listfile.csv\")\n",
    "update_listfile_by_subject_ids(test_id, new_test_list, \"../data/test/test_listfile.csv\")\n",
    "update_listfile_by_subject_ids(validation_id, new_val_list, \"../data/train/val_listfile.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 将每个 subject_id 的笔记分配到对应的 train/test 文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['DISCHTIME'] = pd.to_datetime(notes['DISCHTIME'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['CHARTDATE'] = pd.to_datetime(notes['CHARTDATE'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:151: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['CHARTTIME'] = pd.to_datetime(notes['CHARTTIME'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['DISCHDATE'] = notes['DISCHTIME'].values.astype('<M8[D]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT 26865 of 26865...DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['DISCHTIME'] = pd.to_datetime(notes['DISCHTIME'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['CHARTDATE'] = pd.to_datetime(notes['CHARTDATE'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:151: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['CHARTTIME'] = pd.to_datetime(notes['CHARTTIME'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['DISCHDATE'] = notes['DISCHTIME'].values.astype('<M8[D]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT 3345 of 3345...DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['DISCHTIME'] = pd.to_datetime(notes['DISCHTIME'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['CHARTDATE'] = pd.to_datetime(notes['CHARTDATE'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:151: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['CHARTTIME'] = pd.to_datetime(notes['CHARTTIME'])\n",
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_11356\\1285592341.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  notes['DISCHDATE'] = notes['DISCHTIME'].values.astype('<M8[D]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT 3363 of 3363...DONE!\n"
     ]
    }
   ],
   "source": [
    "#make a dir for each patient\n",
    "break_up_notes_by_subject(notes_train, OUTPUT_PATH_train)\n",
    "break_up_notes_by_subject(notes_test, OUTPUT_PATH_test)\n",
    "break_up_notes_by_subject(notes_validation, OUTPUT_PATH_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 生成用于词嵌入的总笔记文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedlist = new_train_list + new_val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedlist = sorted(joinedlist, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notes_word_embeddings = create_train_val_test_notes(notes, joinedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_note = ''.join(notes_word_embeddings['TEXT'].dropna().astype(str).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = split_doc(dump_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(\"../data/dump_notes\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dump_note_filename = 'dump_notes.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(\"../data/dump_notes\", dump_note_filename), 'w')\n",
    "for sent in sentences:\n",
    "    #tokenize and remove strange token with regex as per the separated function shared for mimi3csv.py\n",
    "    sent = _preprocess1(sent)\n",
    "    cleaned_tokens = tokenize(sent)\n",
    "    #output format: one sentence per line\n",
    "    if len(cleaned_tokens) > 0:\n",
    "        for t in cleaned_tokens:\n",
    "            f.write(t + \" \")\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

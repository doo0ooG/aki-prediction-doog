{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-07 05:06:59.955047\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import gc\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2设置一些常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full input files pathes are here\n",
    "DATA_PATH_stages=\"../data/kdigo_stages_measured.csv\" \n",
    "DATA_PATH_labs = \"../data/labs-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vitals = \"../data/vitals-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vents = \"../data/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_detail=\"../data/icustay_detail-kdigo_stages_measured.csv\" \n",
    "SEPARATOR=\";\"\n",
    "\n",
    "# 输出路径\n",
    "OUTPUT_PATH = \"../data/AKI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数为常量\n",
    "\n",
    "# 决定哪个分类任务将被执行, 每次只执行一个分类任务\n",
    "ALL_STAGES = False # 如果为True，则执行多分类（0， 1， 2， 3）而不是二分类\n",
    "\n",
    "CLASS1 = True     # 是否开启AKI的二分类，就是aki>=1都是1,否则为0\n",
    "CLASS2 = False    # 是否开启中等AKI的二分类，就是aki>=2都是1,否则为0\n",
    "CLASS3 = False    # 是否开启重度AKI的二分类，就是aki>=3都是1,否则为0\n",
    "\n",
    "MAX_FEATURE_SET = True\n",
    "\n",
    "\n",
    "# 数据重采样与缺失值处理\n",
    "TIME_SAMPLING = True \n",
    "SAMPLING_INTERVAL = '1H'\n",
    "# RESAMPLE_LIMIT = 16 # 4 days * 6h interval\n",
    "\n",
    "# MOST_COMMON=False 表示：数值型变量用平均值（mean）填；类别型变量用最大值（出现最多的值）填。\n",
    "MOST_COMMON = False \n",
    "\n",
    "# 模拟 ICU 预测任务的设定 —— 只使用前 48 小时的数据。\n",
    "MAX_HOUR = 48\n",
    "\n",
    "IMPUTE_EACH_ID = True           # 对每个 ICU stay 单独填补缺失值\n",
    "IMPUTE_COLUMN = False           # 不用全列的统计信息填\n",
    "IMPUTE_METHOD = 'most_frequent' \n",
    "FILL_VALUE = 0                  # 用于填充 3D数组中不规则位置（例如被Pad的地方）\n",
    "\n",
    "# 年龄筛选（只用成年患者）\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = -1\n",
    "\n",
    "# 归一化与截断（异常值处理）\n",
    "# 使用最常见的 min-max 归一化。\n",
    "NORMALIZATION = 'min-max'\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "# 对特征做截断，把上/下 1% 的极端值剪掉，处理异常值。\n",
    "CAPPING = True\n",
    "if CAPPING:\n",
    "    CAPPING_THRESHOLD_UPPER = 0.99\n",
    "    CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "\n",
    "# 使用随机划分训练/验证/测试集（而不是预设好的划分）\n",
    "# 20% 数据用于验证或测试集。\n",
    "# 随机种子是 42（保证可复现性）。\n",
    "RANDOM_SPLIT = True\n",
    "FIXED = False\n",
    "RANDOM_SEED = 42\n",
    "SPLIT_SIZE = 0.2 \n",
    "\n",
    "# 特征列表定义\n",
    "\n",
    "# min_set 是一个最小特征集，包含基础变量：id，时间点，肌酐，尿量，AKI阶段等。\n",
    "min_set =  [\"icustay_id\", \"charttime\", \"creat\", \"uo_rt_6hr\", \"uo_rt_12hr\", \"uo_rt_24hr\", \"aki_stage\"]\n",
    "\n",
    "\n",
    "# max_set 是一个扩展特征集，包含了更多实验指标、生理参数、性别、种族、入院类型等信息。\n",
    "# 当前 MAX_FEATURE_SET = True，所以模型会使用 max_set\n",
    "max_set = ['icustay_id', 'charttime', 'aki_stage', 'hadm_id', 'albumin_avg','aniongap_avg', 'bicarbonate_avg', \n",
    "           'bilirubin_avg', 'bun_avg','chloride_avg', 'creat', 'diasbp_mean', 'glucose_avg', 'heartrate_mean',\n",
    "           'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', 'resprate_mean','sodium_avg', 'spo2_mean', 'sysbp_mean', \n",
    "           'uo_rt_12hr', 'uo_rt_24hr','uo_rt_6hr', 'wbc_avg', 'sedative', 'vasopressor', 'vent', 'age', 'F','M', \n",
    "           'asian', 'black', 'hispanic', 'native', 'other', 'unknown','white', 'ELECTIVE', 'EMERGENCY', 'URGENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3固定随机种子，保证结果可复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定随机种子, 保证可复现性\n",
    "def same_seed(seed = 42): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "same_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4设置一些全局函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    \"\"\"\n",
    "    对数值型特征进行异常值截断处理，将其限制在指定的分位数范围内。\n",
    "    不处理的列包括 ID 和时间相关字段。\n",
    "    \"\"\"\n",
    "    print(f\"Capping between the {CAPPING_THRESHOLD_LOWER} and {CAPPING_THRESHOLD_UPPER} quantiles.\")\n",
    "    \n",
    "    # 不参与截断的列\n",
    "    exclude_columns = ['icustay_id', 'charttime', 'aki_stage', 'subject_id', 'intime', 'HOURS']\n",
    "    # 筛选需要截断的列\n",
    "    target_columns = df.columns.difference(exclude_columns)\n",
    "    # 分位数截断\n",
    "    lower_bound = df[target_columns].quantile(CAPPING_THRESHOLD_LOWER)\n",
    "    upper_bound = df[target_columns].quantile(CAPPING_THRESHOLD_UPPER)\n",
    "    \n",
    "    df[target_columns] = df[target_columns].clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "    \n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    \"\"\"\n",
    "    对指定列使用 Min-Max 归一化，将值缩放至 [0, 1]。\n",
    "    \"\"\"\n",
    "    print(f\"Normalizing in [0,1] using {NORMALIZATION} normalization.\")\n",
    "    \n",
    "    df[norm_mask] = (df[norm_mask] - df[norm_mask].min()) / (df[norm_mask].max() - df[norm_mask].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\"\n",
    "    对缺失值列按 key_cols 分组计算众数(不含 NaN),用于填充。\n",
    "    若有多个众数，仅保留排序最靠前的。\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(key_cols + [value_col]).size().to_frame('counts').reset_index()\n",
    "    top_values = grouped.sort_values('counts', ascending=False)\n",
    "    mode_per_group = top_values.drop_duplicates(subset=key_cols).drop(columns='counts')\n",
    "    return mode_per_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.读取csv文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 就是读csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取 CSV 文件\n",
      "将 charttime 转换为时间戳\n"
     ]
    }
   ],
   "source": [
    "# 读取主数据集（包含AKI阶段、肌酐、尿量等）\n",
    "print(\"读取 CSV 文件\")\n",
    "X = pd.read_csv(DATA_PATH_stages, sep=SEPARATOR)\n",
    "\n",
    "# 删除冗余列，仅保留融合后的 aki_stage 标签\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis=1, inplace=True)\n",
    "\n",
    "# 删除5个关键字段全部缺失的行（这些行无任何有效信息）\n",
    "X = X.dropna(how='all', subset=['creat', 'uo_rt_6hr', 'uo_rt_12hr', 'uo_rt_24hr', 'aki_stage'])\n",
    "\n",
    "# 将 charttime 字段转换为 pandas 的 datetime 类型，便于后续时间处理\n",
    "print(\"将 charttime 转换为时间戳\")\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将 intime 转换为时间戳\n"
     ]
    }
   ],
   "source": [
    "# 读取病人基本信息（包含 icustay_id、入院时间 intime、出生时间等）\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "\n",
    "# 删除与建模无关的静态信息，仅保留后续所需的字段（如 intime）\n",
    "dataset_detail.drop([\n",
    "    'dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity',\n",
    "    'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay',\n",
    "    'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay'\n",
    "], axis=1, inplace=True)\n",
    "\n",
    "# 将入ICU时间（intime）转换为 datetime 类型，便于计算时间差\n",
    "print(\"将 intime 转换为时间戳\")\n",
    "dataset_detail['intime'] = pd.to_datetime(dataset_detail['intime'])\n",
    "\n",
    "# dataset_detail的admission_age修改为age，原因我也不知道为什么MIT给的和我们参考的这个代码不一样\n",
    "dataset_detail.rename(columns={'admission_age': 'age'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造 INTIME 表，用于后续将 ICU stay 的时间与数据记录做关联\n",
    "INTIME = pd.DataFrame()\n",
    "INTIME['icustay_id'] = dataset_detail['icustay_id']\n",
    "INTIME['intime'] = dataset_detail['intime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep= SEPARATOR) # 'bands lactate platelet ptt inr pt\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime'])\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "if  MAX_FEATURE_SET:\n",
    "    dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep= SEPARATOR)  \n",
    "    \n",
    "    dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\",\"sysbp_min\", \"sysbp_max\",\"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min','meanbp_max', 'meanbp_mean','tempc_min', 'tempc_max', 'tempc_mean',\n",
    "                        \"resprate_min\", \"resprate_max\", \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis = 1, inplace = True)    \n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "    dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "if  MAX_FEATURE_SET:\n",
    "    dataset_vents = pd.read_csv(DATA_PATH_vents , sep= SEPARATOR)\n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "    dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1简单处理这几张csv表格，并将所有时间序列相关数据合成成一张大表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute avg from min/max in labs file\n",
      "2025-05-07 04:14:25.486678\n",
      "Total records changed: 4316\n",
      "2025-05-07 04:14:26.154707\n"
     ]
    }
   ],
   "source": [
    "# labs的数据冗余大，且有些列是重复的（如 min/max），\n",
    "# 需要对 labs 数据进行处理，计算 min/max 的平均值，并删除原有的 min/max 列\n",
    "print(\"compute avg from min/max in labs file\")\n",
    "print(datetime.now())\n",
    "# 并且从索引 4 开始的22列（共11对）为需要计算平均值的列，格式为：min, max, min, max, ...\n",
    "\n",
    "\n",
    "df_info = dataset_labs.iloc[:, :4].copy() # 保存前4列数据不变\n",
    "lab_cols = dataset_labs.columns[4:4+22] # 获取需要处理的22列\n",
    "new_lab_cols = [] # 新建一个列表存放经过平均值处理后的新列数据\n",
    "null_l = [] # 用来记录出现一侧为缺失而另一侧非缺失的行索引\n",
    "changed = 0 # 用于累计更改的记录数\n",
    "\n",
    "# 分组处理，每组两列，表示一项指标的min和max\n",
    "for i in range(0, len(lab_cols), 2):\n",
    "    col_min_name = lab_cols[i]\n",
    "    col_max_name = lab_cols[i+1]\n",
    "    \n",
    "    col_min = dataset_labs[col_min_name]\n",
    "    col_max = dataset_labs[col_max_name]\n",
    "    \n",
    "    # 计算平均值：(min + max)/2，注意如果其中一个是NaN，结果仍为NaN\n",
    "    avg_val = (col_min + col_max) / 2\n",
    "    \n",
    "    # 判断条件：如果二者相等或均为NaN，则不更新；否则更新为平均值\n",
    "    cond = ~((col_min == col_max) | (col_min.isna() & col_max.isna()))\n",
    "    \n",
    "    # 记录更改的个数\n",
    "    changed_pair = cond.sum()\n",
    "    changed += changed_pair\n",
    "    \n",
    "    # 若其中一边为NaN而另一边不为NaN，则记录该行索引\n",
    "    null_condition = ((col_min.isna() & col_max.notna()) | (col_min.notna() & col_max.isna()))\n",
    "    null_rows = dataset_labs.index[null_condition].tolist()\n",
    "    null_l.extend(null_rows)\n",
    "    \n",
    "    # 创建新列，只有在 cond 条件下更新为平均值，否则保持原min值\n",
    "    new_series = col_min.copy()\n",
    "    new_series[cond] = avg_val[cond]\n",
    "    # 添加到新列列表中\n",
    "    new_lab_cols.append(new_series)\n",
    "\n",
    "# 构造新的 DataFrame，将原来的前4列与新计算的11列合并\n",
    "new_dataset_labs = pd.concat([df_info] + new_lab_cols, axis=1)\n",
    "# 重新设置列名称（后面的顺序与论文中对应的实验指标名称相同）\n",
    "new_dataset_labs.columns = ['subject_id', 'hadm_id', 'icustay_id', 'charttime',\n",
    "                              'aniongap_avg', 'bicarbonate_avg', 'creatinine_avg', 'chloride_avg',\n",
    "                              'glucose_avg', 'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg',\n",
    "                              'sodium_avg', 'bun_avg', 'wbc_avg']\n",
    "dataset_labs = new_dataset_labs\n",
    "\n",
    "if len(null_l) > 0:\n",
    "    print(\"null values encountered\")\n",
    "\n",
    "print(\"Total records changed:\", changed)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里是多数据源数据，考虑合并，为什么呢？**\n",
    "\n",
    "以本研究中用于预测急性肾损伤（AKI）的关键变量——肌酐（Creatinine）为例，其在 \n",
    "数据源中分别以 creatinine_avg 和 creat 的形式在不同视图中记\n",
    "录。这两者分别来源于聚合后的实验室均值视图与原始实验室检验表，尽管其本\n",
    "质上指向相同的生理指标，但在实际应用中存在显著差异。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并肌酐(creatinine)与葡萄糖(glucose)数据\n"
     ]
    }
   ],
   "source": [
    "print(\"合并肌酐(creatinine)与葡萄糖(glucose)数据\")\n",
    "# ==== 肌酐处理 ====\n",
    "# 提取实验室数据中的 creatinine（非缺失）\n",
    "creat_l = dataset_labs[['icustay_id', 'charttime', 'creatinine_avg']].dropna(subset=['creatinine_avg'])\n",
    "creat_l.rename(columns={\"creatinine_avg\": \"creat\"}, inplace=True)\n",
    "# 提取标签数据 X 中的 creat（非缺失）\n",
    "creat_x = X[['icustay_id', 'charttime', 'creat']].dropna(subset=['creat'])\n",
    "# 合并两个来源的数据，并去重\n",
    "creat_merged = pd.concat([creat_x, creat_l], ignore_index=True).drop_duplicates()\n",
    "# 删除旧的 creat 字段\n",
    "X.drop(columns=['creat'], inplace=True)\n",
    "dataset_labs.drop(columns=['creatinine_avg'], inplace=True)\n",
    "# 删除 labs 中除了 ID 和时间外全为空的行（表示该时间点无任何实验室记录）\n",
    "dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all', inplace=True)\n",
    "# 合并新的 creat 回 X\n",
    "X = pd.merge(X, creat_merged, on=[\"icustay_id\", \"charttime\"], how=\"outer\", sort=True, copy=False)\n",
    "\n",
    "\n",
    "\n",
    "# ==== 葡萄糖处理 ====\n",
    "if MAX_FEATURE_SET:\n",
    "    # 提取 Vitals 中的 glucose（非缺失）\n",
    "    glucose_v = dataset_vitals[\n",
    "        ['subject_id', 'hadm_id', 'icustay_id', 'charttime', 'glucose_mean']\n",
    "    ].dropna(subset=['glucose_mean'])\n",
    "    glucose_v.rename(columns={\"glucose_mean\": \"glucose_avg\"}, inplace=True)\n",
    "    # 提取 Labs 中的 glucose（非缺失）\n",
    "    glucose_l = dataset_labs[\n",
    "        ['subject_id', 'hadm_id', 'icustay_id', 'charttime', 'glucose_avg']\n",
    "    ].dropna(subset=['glucose_avg'])\n",
    "    # 合并 vitals 与 labs 中的 glucose，并去重\n",
    "    glucose_merged = pd.concat([glucose_l, glucose_v], ignore_index=True).drop_duplicates()\n",
    "    # 删除原始 glucose 字段\n",
    "    dataset_labs.drop(columns=['glucose_avg'], inplace=True)\n",
    "    dataset_vitals.drop(columns=['glucose_mean'], inplace=True)\n",
    "    # 删除 vitals 中除了 ID 和时间外全为空的行\n",
    "    dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all', inplace=True)\n",
    "    # 合并 glucose_avg 到 dataset_labs\n",
    "    dataset_labs = pd.merge(\n",
    "        dataset_labs, glucose_merged,\n",
    "        on=['subject_id', 'hadm_id', 'icustay_id', 'charttime'],\n",
    "        how=\"outer\", sort=True, copy=False\n",
    "    )\n",
    "\n",
    "\n",
    "# 最后，对两个主要数据集按 icustay_id + charttime 排序\n",
    "X.sort_values(by=['icustay_id', 'charttime'], inplace=True, ignore_index=True)\n",
    "dataset_labs.sort_values(by=['icustay_id', 'charttime'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging labs, vitals and vents files\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "if MAX_FEATURE_SET:\n",
    "    X = pd.merge(X, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 回收dataset_labs，dataset_vitals，dataset_vents的内存\n",
    "del dataset_labs, dataset_vitals, dataset_vents\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 筛选年龄、住院时长、构造标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，年龄必须成年，考虑的因素？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理与时间相关的动态数据\n",
      "即将过滤掉未成年患者。\n"
     ]
    }
   ],
   "source": [
    "print(\"开始处理与时间相关的动态数据\") \n",
    "print(\"即将过滤掉未成年患者。\")\n",
    "dataset_detail = dataset_detail.loc[dataset_detail['age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "adults_icustay_id_list = np.sort(adults_icustay_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其次，每条icustay_id所对应的记录时长必须大于48h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "即将剔除 charttime 范围小于 48 小时的 ICU stay\n",
      "2267 long stays\n",
      "这里有 5506 条id对应的记录时间跨度小于48h\n"
     ]
    }
   ],
   "source": [
    "print(\"即将剔除 charttime 范围小于 48 小时的 ICU stay\")\n",
    "def more_than_HOURS_ahead(X):\n",
    "    # 初始化列表用于记录不同类型的 ICU stay\n",
    "    drop_list = []             # charttime 跨度 < 48 小时的 ICU stay\n",
    "    los_list = []              # 所有保留 ICU stay 的住院时长（单位：天）\n",
    "    long_stays_id = []         # 住院时间超过 35 天的 ICU stay\n",
    "    last_charttime_list = []   # 与 long_stays_id 对应的最后 charttime\n",
    "\n",
    "    # 计算每个 icustay_id 的首末 charttime，并求时间跨度\n",
    "    stay_span_df = (\n",
    "        X.groupby(\"icustay_id\")[\"charttime\"]\n",
    "         .agg(['first', 'last'])  # 取每个 stay 的首末时间\n",
    "         .reset_index()\n",
    "    )\n",
    "\n",
    "    # 计算 LOS（天）\n",
    "    stay_span_df['los'] = (\n",
    "        (stay_span_df['last'] - stay_span_df['first'])\n",
    "        .dt.total_seconds() / 60 / 60 / 24  # 转换成天\n",
    "    ).round(4)\n",
    "\n",
    "    # 遍历每条 ICU stay 记录\n",
    "    for _, row in stay_span_df.iterrows():\n",
    "        icustay_id = row['icustay_id']\n",
    "        los = row['los']\n",
    "\n",
    "        if los < 48 / 24:  # 少于 48 小时\n",
    "            drop_list.append(icustay_id)\n",
    "        else:\n",
    "            los_list.append(los)\n",
    "            if los > 35:\n",
    "                long_stays_id.append(icustay_id)\n",
    "                last_charttime_list.append(row['last'])\n",
    "\n",
    "    print(f\"{len(long_stays_id)} long stays\")\n",
    "    print(f\"这里有 {len(drop_list)} 条id对应的记录时间跨度小于48h\")\n",
    "\n",
    "    # 从 X 中删除 ICU stay < 48 小时的记录\n",
    "    X = X[~X['icustay_id'].isin(drop_list)].copy()\n",
    "\n",
    "    # 重新获取剩下的 ICU stay ID，并按时间排序\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index=True)\n",
    "\n",
    "    return id_list, X, long_stays_id, last_charttime_list\n",
    "\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们参考的论文只进行简单的二分类，并且aki阶段为1，2，3均设置为1，否则为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarise labels\n"
     ]
    }
   ],
   "source": [
    "print(\"binarise labels\")\n",
    "if ALL_STAGES:\n",
    "    pass\n",
    "elif CLASS1:\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS2:\n",
    "    X.loc[X['aki_stage'] < 2, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS3:\n",
    "    X.loc[X['aki_stage'] < 3, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 2, 'aki_stage'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对于每个 ICU stay,只要在任意时刻出现过 AKI = 1,就将该 stay 的最终标签设为 1,否则为 0。\n"
     ]
    }
   ],
   "source": [
    "print(\"对于每个 ICU stay,只要在任意时刻出现过 AKI = 1,就将该 stay 的最终标签设为 1,否则为 0。\")\n",
    "def one_label_per_icustay(id_list, X):\n",
    "    # 只筛选需要的列：icustay_id 和 aki_stage\n",
    "    aki_df = X[['icustay_id', 'aki_stage']]\n",
    "\n",
    "    # 按 icustay_id 分组，检查该组中是否有任何一个值等于 1\n",
    "    label_df = (\n",
    "        aki_df.groupby('icustay_id')['aki_stage']\n",
    "              .apply(lambda x: (x == 1).any())\n",
    "              .astype(int)  # True -> 1, False -> 0\n",
    "              .reset_index()\n",
    "              .rename(columns={'aki_stage': 'y_true'})\n",
    "    )\n",
    "\n",
    "    # 保证输出的 icustay_id 顺序与 id_list 保持一致\n",
    "    label_df = label_df.set_index('icustay_id').reindex(id_list).reset_index()\n",
    "\n",
    "    return label_df['y_true'].tolist()\n",
    "\n",
    "target_list = one_label_per_icustay(id_list, X)\n",
    "\n",
    "target = pd.DataFrame()\n",
    "target['icustay_id'] = id_list\n",
    "target['y_true'] = target_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of neg and pos label in target(whole stay)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y_true\n",
       "1    31284\n",
       "0    15963\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"number of neg and pos label in target(whole stay)\")\n",
    "target['y_true'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset drop AKI stages column\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset drop AKI stages column\")\n",
    "X = X.drop(['aki_stage'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['subject_id', 'hadm_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "icustay_id                0\n",
       "charttime                35\n",
       "uo_rt_6hr           7913979\n",
       "uo_rt_12hr          7913979\n",
       "uo_rt_24hr          7913979\n",
       "creat              10288441\n",
       "aniongap_avg       10316556\n",
       "bicarbonate_avg    10305236\n",
       "chloride_avg       10239953\n",
       "hematocrit_avg     10123189\n",
       "hemoglobin_avg     10256761\n",
       "potassium_avg      10037843\n",
       "sodium_avg         10202743\n",
       "bun_avg            10292666\n",
       "wbc_avg            10349001\n",
       "glucose_avg         9216948\n",
       "heartrate_mean      4934900\n",
       "sysbp_mean          5383127\n",
       "diasbp_mean         5384801\n",
       "resprate_mean       4899036\n",
       "spo2_mean           5081490\n",
       "vent                     35\n",
       "vasopressor              35\n",
       "sedative                 35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 重新采样数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "假设原始 ICU 数据如下（间隔不规律）：\n",
    "\n",
    "| icustay\\_id | charttime           | glucose | vasopressor |\n",
    "| ----------- | ------------------- | ------- | ----------- |\n",
    "| 100001      | 2020-01-01 01:27:00 | 150     | 0           |\n",
    "| 100001      | 2020-01-01 01:55:00 | 130     | 1           |\n",
    "| 100001      | 2020-01-01 03:10:00 | 140     | 0           |\n",
    "\n",
    "重采样（按 1 小时）后将变为：\n",
    "\n",
    "| icustay\\_id | charttime           | glucose | vasopressor |\n",
    "| ----------- | ------------------- | ------- | ----------- |\n",
    "| 100001      | 2020-01-01 01:00:00 | 140     | 1           |\n",
    "| 100001      | 2020-01-01 02:00:00 | NaN     | NaN         |\n",
    "| 100001      | 2020-01-01 03:00:00 | 140     | 0           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间重采样：每 1H 一次\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doog\\AppData\\Local\\Temp\\ipykernel_3124\\1014288339.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重采样完成，当前数据形状：(13365741, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"开始时间重采样：每 {SAMPLING_INTERVAL} 一次\")\n",
    "\n",
    "# 设置不参与聚合的列\n",
    "skip = ['icustay_id', 'charttime']\n",
    "if MAX_FEATURE_SET:\n",
    "    discrete_feat = ['sedative', 'vasopressor', 'vent']\n",
    "    skip.extend(discrete_feat)\n",
    "else:\n",
    "    discrete_feat = []\n",
    "\n",
    "# 推断数值特征（排除 ID、时间和离散特征）\n",
    "numeric_feat = list(X.columns.difference(skip))\n",
    "\n",
    "# 设定重采样方法\n",
    "X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "\n",
    "# 分别处理 numeric 和 discrete 特征\n",
    "resampled_parts = []\n",
    "\n",
    "if numeric_feat:\n",
    "    X_numeric = X[numeric_feat].mean()\n",
    "    resampled_parts.append(X_numeric)\n",
    "\n",
    "if discrete_feat:\n",
    "    X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    resampled_parts.append(X_discrete)\n",
    "\n",
    "# 合并数值与离散部分（如果有）\n",
    "X = pd.concat(resampled_parts, axis=1).reset_index()\n",
    "\n",
    "print(f\"重采样完成，当前数据形状：{X.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按 Yereva 项目的时间窗口设定，只保留 ICU 前 48 小时内的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在将 ICU 入院时间(intime)合并到主数据 X 中\n",
      "正在删除 ICU 内时间超出 48 小时或早于入院时间的记录(HOURS < 0 或 > 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"正在将 ICU 入院时间(intime)合并到主数据 X 中\")\n",
    "print(\"正在删除 ICU 内时间超出 48 小时或早于入院时间的记录(HOURS < 0 或 > 48)\")\n",
    "X = pd.merge(X, INTIME, on=[\"icustay_id\"], how=\"left\", copy=False)# 合并入 ICU 时间\n",
    "X['HOURS'] = (X.charttime - X.intime).apply(lambda s: s / np.timedelta64(1, 's')) / 3600 # 计算每条记录距离入 ICU 的时间（单位：小时）\n",
    "X = X[X['HOURS'] >= 0] # 删除 ICU 入住前的数据（负时间）\n",
    "X = X[X['HOURS'] <= MAX_HOUR] # 删除超出 48 小时范围的数据（只保留前 MAX_HOUR 小时）\n",
    "X = X.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246888, 26)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386827     0.22\n",
       "386828     1.22\n",
       "386829     2.22\n",
       "386830     3.22\n",
       "386831     4.22\n",
       "386832     5.22\n",
       "386833     6.22\n",
       "386834     7.22\n",
       "386835     8.22\n",
       "386836     9.22\n",
       "386837    10.22\n",
       "386838    11.22\n",
       "386839    12.22\n",
       "386840    13.22\n",
       "386841    14.22\n",
       "386842    15.22\n",
       "386843    16.22\n",
       "386844    17.22\n",
       "386845    18.22\n",
       "386846    19.22\n",
       "386847    20.22\n",
       "386848    21.22\n",
       "386849    22.22\n",
       "386850    23.22\n",
       "386851    24.22\n",
       "386852    25.22\n",
       "386853    26.22\n",
       "386854    27.22\n",
       "386855    28.22\n",
       "386856    29.22\n",
       "386857    30.22\n",
       "386858    31.22\n",
       "386859    32.22\n",
       "386860    33.22\n",
       "386861    34.22\n",
       "386862    35.22\n",
       "386863    36.22\n",
       "386864    37.22\n",
       "386865    38.22\n",
       "386866    39.22\n",
       "386867    40.22\n",
       "386868    41.22\n",
       "386869    42.22\n",
       "386870    43.22\n",
       "386871    44.22\n",
       "386872    45.22\n",
       "386873    46.22\n",
       "386874    47.22\n",
       "Name: HOURS, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if follows one hour interval\n",
    "# X.loc[X[\"icustay_id\"]== 272725].sort_values(by=['HOURS'])[\"HOURS\"]\n",
    "# X.loc[X[\"icustay_id\"]== 244882].sort_values(by=['HOURS'])[\"HOURS\"]\n",
    "X.loc[X[\"icustay_id\"]== 217128].sort_values(by=['HOURS'])[\"HOURS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 插值（缺失值填充）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputation.\")\n",
    "remove_list = ['icustay_id','charttime','intime','HOURS']\n",
    "\n",
    "# 对 ICU 数据中的缺失值（NaN）进行按 ICU stay（icustay_id）内众数填充（most common value\n",
    "if IMPUTE_EACH_ID:\n",
    "    column_name = list(X.columns)\n",
    "    for item in remove_list:\n",
    "        column_name.remove(item)\n",
    "    for feature in column_name:\n",
    "        X.loc[X[feature].isnull(), feature] = X.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# 以下代码不走我就注释了\n",
    "# # imputation based on whole column\n",
    "# if IMPUTE_COLUMN:\n",
    "#     imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "#     cols = list(X.columns)\n",
    "#     for item in remove_list:\n",
    "#         cols.remove(item)\n",
    "#     X[cols]=imp.fit_transform(X[cols])  \n",
    "\n",
    "# 对剩余仍为 NaN 的值填充为 FILL_VALUE（兜底\n",
    "X = X.fillna(FILL_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check variables\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "# more comfortable to review in this order\n",
    "print(\"check variables\")\n",
    "try:\n",
    "    cols = ['icustay_id', 'charttime','aniongap_avg','bicarbonate_avg', 'bun_avg','chloride_avg',\n",
    "            'creat','diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg','hemoglobin_avg', \n",
    "            'potassium_avg', 'resprate_mean', 'sodium_avg','spo2_mean', 'sysbp_mean', 'uo_rt_12hr', \n",
    "            'uo_rt_24hr', 'uo_rt_6hr','wbc_avg', 'sedative', 'vasopressor', 'vent',\"HOURS\" , \"intime\"]\n",
    "    X = X[cols]\n",
    "    print(\"success\")\n",
    "except:\n",
    "    try:\n",
    "        cols = ['icustay_id', 'charttime','creat','uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr']\n",
    "        X = X[cols]\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246888, 26)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 构造类别特征（独热编码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_FEATURE_SET:\n",
    "    #extract datasets based on id_list\n",
    "    dataset_detail = dataset_detail.loc[dataset_detail['icustay_id'].isin(id_list)]\n",
    "    #sort by ascending order\n",
    "    dataset_detail = dataset_detail.sort_values(by=['icustay_id'])\n",
    "    #print(dataset_detail)\n",
    "    \n",
    "    #transfrom categorical data to binary form\n",
    "    dataset_detail = dataset_detail.drop(['intime'], axis=1)\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('gender')))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop(\"ethnicity_grouped\")))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('admission_type')))\n",
    "    #X = X.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    #dataset_detail = dataset_detail.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    X =  pd.merge(X, dataset_detail, on = [\"icustay_id\"], how= \"left\", copy = False) \n",
    "\n",
    "    numeric_feat.append('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['charttime', 'intime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将X的后12个变量改为float32类型\n",
    "X.iloc[:, -12:] = X.iloc[:, -12:].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names =['Anion gap', 'Bicarbonate', 'Blood Urea Nitrogen', 'Chloride', 'Creatinine', 'Diastolic BP', 'Glucose', 'Heart rate', \n",
    "            'Hematocrit', 'Hemoglobin', 'Potassium', 'Respiratory rate', 'Sodium', 'Oxygen saturation', 'Systolic BP', 'Urine output 12h', 'Urine output 24h', 'Urine output 6h',\n",
    "            'White cell count', 'Sedative', 'Vasopressor', 'Ventilation', 'Age', 'Female gender', 'Male gender', 'Asian ethnicity', 'Black ethnicity', 'Hispanic ethnicity', 'Native american', \n",
    "            'Other ethnicity', 'Ethnicity unknown', 'White ethnicity', 'Elective admission', 'Emergency admission', 'Urgent admission']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 剔除极端值，标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capping between the 0.01 and 0.99 quantiles.\n",
      "Normalizing in [0,1] using min-max normalization.\n"
     ]
    }
   ],
   "source": [
    "X = cap_data(X)\n",
    "\n",
    "X = normalise_data(X, numeric_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icustay_id    299999\n",
      "size              49\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sort_values(by=['subject_id', 'HOURS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算算最终参与数据集构造的数据特征量多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['icustay_id', 'aniongap_avg', 'bicarbonate_avg', 'bun_avg', 'chloride_avg', 'creat', 'diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', 'resprate_mean', 'sodium_avg', 'spo2_mean', 'sysbp_mean', 'uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr', 'wbc_avg', 'sedative', 'vasopressor', 'vent', 'HOURS', 'subject_id', 'age', 'F', 'M', 'asian', 'black', 'hispanic', 'native', 'other', 'unknown', 'white', 'ELECTIVE', 'EMERGENCY', 'URGENT']\n",
      "number of features: 35\n"
     ]
    }
   ],
   "source": [
    "features_list = list(X.columns)\n",
    "print(features_list)\n",
    "# list of variables to be removed at the end\n",
    "remove_list_final = ['icustay_id', 'subject_id', 'F']\n",
    "for item in remove_list_final:\n",
    "    features_list.remove(item)\n",
    "\n",
    "features = len(features_list)\n",
    "print(\"number of features: \" + str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 按照subject_id随机划分训练集，验证集，测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract subject_id list\n"
     ]
    }
   ],
   "source": [
    "if RANDOM_SPLIT:\n",
    "    print(\"extract subject_id list\")\n",
    "    subject_id = X[\"subject_id\"].unique()\n",
    "    subject_id = np.sort(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique subject id: 34280\n"
     ]
    }
   ],
   "source": [
    "if RANDOM_SPLIT:\n",
    "    print(\"number of unique subject id: \" + str(len(subject_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SPLIT\n",
      "divide dataset into train, test and validation sets\n",
      "test is 3428\n",
      "train is 27427\n",
      "val is 3425\n"
     ]
    }
   ],
   "source": [
    "if RANDOM_SPLIT:\n",
    "    print('RANDOM SPLIT')\n",
    "    print(\"divide dataset into train, test and validation sets\")\n",
    "    id_train_val, id_test = train_test_split(subject_id, test_size = 0.1, random_state = RANDOM_SEED) # train set is 80%)\n",
    "    print(\"test is %d\" % len(id_test))\n",
    "    # remaining 20% split in halves as test and validation 10% and 10%\n",
    "    id_train, id_val = train_test_split(id_train_val, test_size = 0.111, random_state = RANDOM_SEED) # test 10% valid 10%\n",
    "    print(\"train is %d\" %len(id_train))\n",
    "    print(\"val is %d\" %len(id_val))\n",
    "\n",
    "    #sort list\n",
    "    id_test.sort()\n",
    "    id_train.sort()\n",
    "    id_val.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 将 icustay 数据转换为单独的时间序列 CSV 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#str(list(target.loc[target['icustay_id'] == 237693]['y_true'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_icustay_to_AKIfolder(dataset, subject_id, output_path, id_train, id_test, id_val, target):\n",
    "    \n",
    "    temp_icustay_list = [] #to store the icustay_id under same subject_id\n",
    "    n = 0 #index to loop through temp_icustay_list\n",
    "    num_stay = 0\n",
    "    temp_dataset = pd.DataFrame()\n",
    "    sub_temp_dataset = pd.DataFrame()\n",
    "    \n",
    "    train_pairs = []\n",
    "    test_pairs = []\n",
    "    val_pairs = []\n",
    "    \n",
    "\n",
    "    for subject in subject_id:\n",
    "        #make path for subject folder\n",
    "        dn = os.path.join(output_path, str(subject))\n",
    "        try:\n",
    "            os.makedirs(dn)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        temp_dataset = dataset.loc[dataset[\"subject_id\"]== subject].sort_values(by=['icustay_id'])\n",
    "        temp_icustay_list = temp_dataset[\"icustay_id\"].unique()\n",
    "        num_stay = len(temp_icustay_list)\n",
    "        n = 0\n",
    "        \n",
    "        while n < num_stay:\n",
    "            sub_temp_dataset = temp_dataset.loc[temp_dataset[\"icustay_id\"]== temp_icustay_list[n]]\n",
    "            sub_temp_dataset = sub_temp_dataset.drop(remove_list_final, axis=1)\n",
    "            sub_temp_dataset = sub_temp_dataset.set_index('HOURS').sort_index(axis=0)\n",
    "             \n",
    "            sub_temp_dataset.to_csv(os.path.join(OUTPUT_PATH, str(subject), '{}_episode{}_timeseries_{}.csv'.format(subject, n+1, temp_icustay_list[n]))\n",
    "                                    ,index_label='Hours')\n",
    "            \n",
    "            #create list for id list for train/test/val\n",
    "            if subject in id_train:\n",
    "                train_pairs.append((str(subject)+ \"_note.txt\", str(subject)+ \"_episode\" + str(n+1)+\"_timeseries_\"+ str(temp_icustay_list[n])+ \".csv\", str(list(target.loc[target['icustay_id'] == temp_icustay_list[n]]['y_true'])[0])))\n",
    "            elif subject in id_test:\n",
    "                test_pairs.append((str(subject)+ \"_note.txt\", str(subject)+ \"_episode\" + str(n+1)+\"_timeseries_\"+ str(temp_icustay_list[n])+ \".csv\", str(list(target.loc[target['icustay_id'] == temp_icustay_list[n]]['y_true'])[0])))\n",
    "            elif subject in id_val:\n",
    "                val_pairs.append((str(subject)+ \"_note.txt\", str(subject)+ \"_episode\" + str(n+1)+\"_timeseries_\"+ str(temp_icustay_list[n])+ \".csv\", str(list(target.loc[target['icustay_id'] == temp_icustay_list[n]]['y_true'])[0])))\n",
    "                \n",
    "            n = n+1\n",
    "    \n",
    "    return train_pairs, test_pairs, val_pairs\n",
    "    \n",
    "    \n",
    "train_pairs, test_pairs, val_pairs = convert_icustay_to_AKIfolder(X, subject_id, OUTPUT_PATH, id_train, id_test, id_val, target)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 移动 subject_timeseries.csv 文件 到 train/test 目录中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_partition(subjects_root_path, patients, partition):\n",
    "    if not os.path.exists(os.path.join(subjects_root_path, partition)):\n",
    "        os.mkdir(os.path.join(subjects_root_path, partition))\n",
    "    for patient in patients:\n",
    "        src = os.path.join(subjects_root_path, str(patient))\n",
    "        dest = os.path.join(subjects_root_path, partition)\n",
    "        for filename in os.listdir(src):\n",
    "            shutil.move(os.path.join(src, str(filename)), dest)\n",
    "        os.rmdir(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_to_partition(OUTPUT_PATH, id_train, \"train\")\n",
    "move_to_partition(OUTPUT_PATH, id_val, \"train\")\n",
    "move_to_partition(OUTPUT_PATH, id_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建 test_listfile.csv、train_listfile.csv、val_listfile.csv，并移动到 AKI 文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_PATH, \"train_listfile.csv\"), \"w\") as listfile:\n",
    "    listfile.write('notes,stay,y_true\\n')\n",
    "    for (n, x, y) in train_pairs:\n",
    "        listfile.write('{},{},{}\\n'.format(n, x, str(y)))\n",
    "with open(os.path.join(OUTPUT_PATH, \"val_listfile.csv\"), \"w\") as listfile:\n",
    "    listfile.write('notes,stay,y_true\\n')\n",
    "    for (n, x, y) in val_pairs:\n",
    "        listfile.write('{},{},{}\\n'.format(n, x, str(y))) \n",
    "        \n",
    "with open(os.path.join(OUTPUT_PATH,  \"test_listfile.csv\"), \"w\") as listfile:\n",
    "    listfile.write('notes,stay,y_true\\n')\n",
    "    for (n, x, y) in test_pairs:\n",
    "        listfile.write('{},{},{}\\n'.format(n, x, str(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
